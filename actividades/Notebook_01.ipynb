{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5123e24d",
   "metadata": {},
   "source": [
    "# Notebook 1 â€” Primer contacto con R (sintaxis bÃ¡sica)\n",
    "\n",
    "**Objetivo:** perder el miedo a R y practicar lo imprescindible: variables, tipos bÃ¡sicos y operaciones.\n",
    "\n",
    "**CÃ³mo trabajar:** ejecuta cada celda (Shift+Enter), lee el resultado y completa los ejercicios.\n",
    "\n",
    "> ðŸ’¡ RecomendaciÃ³n: si algo falla, lee el mensaje de error y revisa la lÃ­nea indicada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401dbd3a",
   "metadata": {},
   "source": [
    "##  Â¿QuÃ© es R?\n",
    "\n",
    "R es un **lenguaje de programaciÃ³n orientado al anÃ¡lisis de datos, estadÃ­stica e Inteligencia Artificial**. A diferencia de otros lenguajes generales, R naciÃ³ pensando en datos desde el primer dÃ­a.\n",
    "\n",
    "* **Origen acadÃ©mico:** Aparece en los 90, inspirado en el lenguaje S, para facilitar la investigaciÃ³n estadÃ­stica.\n",
    "* **Software libre:** Mantenido por la **R Foundation**, es gratuito y cuenta con una comunidad global masiva.\n",
    "* **FilosofÃ­a:** R no naciÃ³ para \"programar apps\", naciÃ³ para **pensar con datos**.\n",
    "\n",
    "### Â¿Por quÃ© R es clave en IA?\n",
    "\n",
    "Aunque Python es muy popular, R sigue siendo el estÃ¡ndar en:\n",
    "\n",
    "* ðŸ“Š **AnÃ¡lisis Exploratorio de Datos (EDA):** Es el mÃ¡s rÃ¡pido para entender tus datos visualmente.\n",
    "* ðŸ“ˆ **EstadÃ­stica Avanzada:** Indispensable para validar modelos (tests de hipÃ³tesis, p-valores).\n",
    "* ðŸ¤– **Machine Learning ClÃ¡sico:** Muchos algoritmos se publican en R antes que en cualquier otro lenguaje.\n",
    "\n",
    "---\n",
    "\n",
    "##  El entorno de desarrollo\n",
    "\n",
    "Para trabajar profesionalmente en Machine Learning con R, necesitamos dos piezas que trabajan juntas: el **motor** y el **editor**.\n",
    "\n",
    "### 1. El motor: R (El lenguaje)\n",
    "\n",
    "Es el \"cerebro\" que hace los cÃ¡lculos. Al instalarlo, preparas el compilador que entiende tus instrucciones.\n",
    "\n",
    "* **Descarga:** Se instala desde el [CRAN](https://cran.r-project.org/).\n",
    "\n",
    "### 2. El editor: Â¿RStudio o VS Code?\n",
    "\n",
    "AquÃ­ es donde elegirÃ¡s tu flujo de trabajo segÃºn tu perfil:\n",
    "\n",
    "#### **OpciÃ³n A: RStudio (El estÃ¡ndar de la industria)**\n",
    "\n",
    "Es un entorno diseÃ±ado especÃ­ficamente para datos. Todo estÃ¡ a la mano: tus variables, tus grÃ¡ficos y tus scripts.\n",
    "\n",
    "* **Ideal para:** Si te enfocas 100% en estadÃ­stica, visualizaciÃ³n avanzada y creaciÃ³n de reportes cientÃ­ficos.\n",
    "* **Punto fuerte:** No requiere configuraciÃ³n; instalas y empiezas a programar.\n",
    "\n",
    "\n",
    "<img src=\"img/rstudio-panes-labeled.jpeg\" width=\"900\" alt=\"rstudio-panes\">\n",
    "\n",
    "#### **OpciÃ³n B: VS Code + Jupyter (El enfoque hÃ­brido)**\n",
    "\n",
    "Si ya vienes de Python o prefieres un editor moderno y ligero.\n",
    "\n",
    "* **Ideal para:** Flujos de trabajo de **Machine Learning** donde mezclas lenguajes (Python, SQL, R) o prefieres trabajar con **Notebooks**.\n",
    "* **Punto fuerte:** Los cuadernos de Jupyter permiten ejecutar bloques de cÃ³digo y ver los resultados (grÃ¡ficos o tablas) justo debajo, facilitando la experimentaciÃ³n rÃ¡pida.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Â¿CÃ³mo se instala correctamente?\n",
    "\n",
    "El orden de los factores sÃ­ altera el producto. Sigue estos pasos para evitar errores:\n",
    "\n",
    "1. **Instala R primero:** Elige la versiÃ³n para tu sistema operativo.\n",
    "2. **Instala tu editor:** Ya sea RStudio o Visual Studio Code (con la extensiÃ³n de R y Jupyter).\n",
    "3. **Compiladores adicionales (Opcional pero recomendado):**\n",
    "* En Windows: Instala **RTools**.\n",
    "* *Â¿Por quÃ©?* Algunos paquetes avanzados de IA estÃ¡n escritos en lenguajes como C++ para ser ultra rÃ¡pidos, y R necesita estos \"traductores\" para instalarlos.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### R vs. Python en el flujo de IA\n",
    "\n",
    "Para que sepas dÃ³nde estÃ¡s pisando, aquÃ­ tienes una comparativa rÃ¡pida de las herramientas equivalentes:\n",
    "\n",
    "| Herramienta | R (Tidyverse) | Python (PyData) |\n",
    "| --- | --- | --- |\n",
    "| **ManipulaciÃ³n de datos** | `dplyr` / `tidyr` | `pandas` / `polars` |\n",
    "| **VisualizaciÃ³n** | `ggplot2` | `matplotlib` / `seaborn` |\n",
    "| **Ecosistema ML** | `tidymodels` / `caret` | `scikit-learn` |\n",
    "| **Cuadernos de notas** | Quarto / RMarkdown | Jupyter Notebooks |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d206df98",
   "metadata": {},
   "source": [
    "## 0) PreparaciÃ³n\n",
    "\n",
    "En este cuaderno asumimos que estÃ¡s usando un **kernel de R** en Jupyter.\n",
    "\n",
    "Instala la extensiÃ³n **\"R\"** de Yuki Ueda\n",
    "\n",
    "Para comprobar que todo va bien, ejecuta la celda siguiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb6d31f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'R version 4.5.2 (2025-10-31)'"
      ],
      "text/latex": [
       "'R version 4.5.2 (2025-10-31)'"
      ],
      "text/markdown": [
       "'R version 4.5.2 (2025-10-31)'"
      ],
      "text/plain": [
       "[1] \"R version 4.5.2 (2025-10-31)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Paquete Instalado Version\n",
      "IRkernel     IRkernel      TRUE   1.3.2\n",
      "corrplot     corrplot     FALSE     N/A\n",
      "NbClust       NbClust     FALSE     N/A\n",
      "MASS             MASS      TRUE  7.3.65\n",
      "factoextra factoextra     FALSE     N/A\n",
      "colorspace colorspace      TRUE   2.1.2\n",
      "sp                 sp     FALSE     N/A\n",
      "sf                 sf     FALSE     N/A\n",
      "ggplot2       ggplot2      TRUE   4.0.2\n",
      "clv               clv     FALSE     N/A\n",
      "tidyverse   tidyverse      TRUE   2.0.0\n",
      "caret           caret      TRUE   7.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "â”€â”€ \u001b[1mAttaching core tidyverse packages\u001b[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\n",
      "\u001b[32mâœ”\u001b[39m \u001b[34mdplyr    \u001b[39m 1.2.0     \u001b[32mâœ”\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.6\n",
      "\u001b[32mâœ”\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.1     \u001b[32mâœ”\u001b[39m \u001b[34mstringr  \u001b[39m 1.6.0\n",
      "\u001b[32mâœ”\u001b[39m \u001b[34mggplot2  \u001b[39m 4.0.2     \u001b[32mâœ”\u001b[39m \u001b[34mtibble   \u001b[39m 3.3.1\n",
      "\u001b[32mâœ”\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.5     \u001b[32mâœ”\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.2\n",
      "\u001b[32mâœ”\u001b[39m \u001b[34mpurrr    \u001b[39m 1.2.1     \n",
      "â”€â”€ \u001b[1mConflicts\u001b[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\n",
      "\u001b[31mâœ–\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31mâœ–\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mâ„¹\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "Loading required package: lattice\n",
      "\n",
      "\n",
      "Attaching package: â€˜caretâ€™\n",
      "\n",
      "\n",
      "The following object is masked from â€˜package:purrrâ€™:\n",
      "\n",
      "    lift\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Core de Tidyverse y Caret cargados correctamente."
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in library(factoextra): there is no package called â€˜factoextraâ€™\n",
     "output_type": "error",
     "traceback": [
      "Error in library(factoextra): there is no package called â€˜factoextraâ€™\nTraceback:\n",
      "1. stop(packageNotFoundError(package, lib.loc, sys.call()))"
     ]
    }
   ],
   "source": [
    "R.version.string\n",
    "\n",
    "# 1. Definimos la lista de paquetes que instalamos en Docker\n",
    "paquetes_check <- c(\n",
    "  \"IRkernel\", \"corrplot\", \"NbClust\", \"MASS\", \"factoextra\", \n",
    "  \"colorspace\", \"sp\", \"sf\", \"ggplot2\", \"clv\", \n",
    "  \"tidyverse\", \"caret\"\n",
    ")\n",
    "\n",
    "# 2. Creamos una tabla con el estado de cada uno\n",
    "verificacion <- data.frame(\n",
    "  Paquete = paquetes_check,\n",
    "  Instalado = paquetes_check %in% installed.packages()[, \"Package\"],\n",
    "  Version = sapply(paquetes_check, function(p) {\n",
    "    if (p %in% installed.packages()[, \"Package\"]) as.character(packageVersion(p)) else \"N/A\"\n",
    "  })\n",
    ")\n",
    "\n",
    "# 3. Mostramos el resultado\n",
    "print(verificacion)\n",
    "\n",
    "# 4. Test de carga crÃ­tico para ML\n",
    "# Si esto falla, hay un problema de dependencias en el Docker\n",
    "library(tidyverse)\n",
    "library(caret)\n",
    "cat(\"\\nâœ… Core de Tidyverse y Caret cargados correctamente.\")\n",
    "\n",
    "# Test visual: Clustering con el dataset Iris\n",
    "library(factoextra)\n",
    "data(iris)\n",
    "km.res <- kmeans(iris[, -5], 3)\n",
    "fviz_cluster(km.res, data = iris[, -5], geom = \"point\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57355e68",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Variables y asignaciÃ³n (`<-`)\n",
    "\n",
    "En Python usÃ¡is `=` para asignar valores a variables.\n",
    "En R, **tambiÃ©n existe `=`**, pero **la forma recomendada por la comunidad es usar `<-`**.\n",
    "\n",
    "Este operador se lee como *â€œrecibeâ€*:\n",
    "\n",
    "> la variable **recibe** el valor que hay a la derecha.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "```r\n",
    "edad <- 19\n",
    "```\n",
    "\n",
    "ðŸ’¡ **Consejo prÃ¡ctico**:\n",
    "En la mayorÃ­a de editores (RStudio, Jupyter, Colab), puedes escribir `<-` pulsando **Alt + -**, asÃ­ es mÃ¡s rÃ¡pido y cÃ³modo.\n",
    "\n",
    "En este curso **usaremos siempre `<-`**, para seguir el estÃ¡ndar profesional de R y evitar confusiones mÃ¡s adelante.\n",
    "\n",
    "Ejecuta la siguiente celda y observa el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100cb929",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Ana'"
      ],
      "text/latex": [
       "'Ana'"
      ],
      "text/markdown": [
       "'Ana'"
      ],
      "text/plain": [
       "[1] \"Ana\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "19"
      ],
      "text/latex": [
       "19"
      ],
      "text/markdown": [
       "19"
      ],
      "text/plain": [
       "[1] 19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nombre <- \"Ana\"\n",
    "edad <- 19 \n",
    "\n",
    "nombre\n",
    "edad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cd125",
   "metadata": {},
   "source": [
    "### ðŸ”§ Ejercicio 1\n",
    "Crea dos variables:\n",
    "- `dataset`: El nombre de un conjunto de datos que vas a usar (por ejemplo: \"Iris\", \"Titanic\" o \"MNIST\").\n",
    "- `tasa_aprendizaje`: Un nÃºmero decimal que represente la velocidad a la que aprende el modelo (por ejemplo: 0.001)\n",
    "\n",
    "DespuÃ©s, **muestra** ambas variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319c36be",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Iris'"
      ],
      "text/latex": [
       "'Iris'"
      ],
      "text/markdown": [
       "'Iris'"
      ],
      "text/plain": [
       "[1] \"Iris\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "3e-04"
      ],
      "text/latex": [
       "3e-04"
      ],
      "text/markdown": [
       "3e-04"
      ],
      "text/plain": [
       "[1] 3e-04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: completa\n",
    "dataset <- \"Iris\"\n",
    "tasa_aprendizaje <- 0.0003\n",
    "\n",
    "dataset\n",
    "tasa_aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5547790c",
   "metadata": {},
   "source": [
    "## 2)  CÃ³mo se estructura el cÃ³digo en R\n",
    "\n",
    "Si vienes de Python, hay una diferencia **muy importante** que debes conocer desde ya:\n",
    "\n",
    "### En R la indentaciÃ³n NO define bloques de cÃ³digo\n",
    "\n",
    "En **Python**, el espacio (la indentaciÃ³n) es lo que le dice al programa quÃ© parte del cÃ³digo pertenece a una condiciÃ³n o un bucle:\n",
    "\n",
    "```python\n",
    "if accuracy >= 5:\n",
    "   clase = \"Positivo\"\n",
    "else:\n",
    "    clase = \"Negativo\"\n",
    "```\n",
    "\n",
    "En **R**, lo que define un bloque son **las llaves `{}`**, no los espacios. El intÃ©rprete de R busca el cierre de la llave para saber dÃ³nde termina la instrucciÃ³n.\n",
    "\n",
    "```r\n",
    "if (accuracy >= 5) {\n",
    "  clase <- \"Positivo\"\n",
    "} else {\n",
    "  clase <- \"Negativo\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Reglas de oro en entornos de Datos:\n",
    "\n",
    "* **Las llaves `{}**` son obligatorias si vas a ejecutar varias acciones tras una condiciÃ³n (por ejemplo, clasificar y luego registrar un log).\n",
    "* **Legibilidad:** En scripts de producciÃ³n de ML, la indentaciÃ³n es sagrada para que otros cientÃ­ficos de datos puedan auditar tu lÃ³gica.\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Ejemplo de \"CÃ³digo CrÃ­ptico\" en ML\n",
    "\n",
    "R permite compactar el cÃ³digo de una forma que en Python serÃ­a imposible (pero que dificulta mucho el *debugging*). Este ejemplo **funciona**, pero es una mala prÃ¡ctica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84d92ae",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: object 'prob' not found\n",
     "output_type": "error",
     "traceback": [
      "Error: object 'prob' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "if(prob>0.5){clase<-\"Spam\"}else{clase<-\"Ham\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15af34",
   "metadata": {},
   "source": [
    "> **Tip de experto:** Si estÃ¡s trabajando en RStudio, puedes usar el atajo `Ctrl + Shift + A` para que el editor limpie y ordene automÃ¡ticamente tus llaves e indentaciones.\n",
    "\n",
    "Ejemplo correcto y profesional:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1131942c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Spam'"
      ],
      "text/latex": [
       "'Spam'"
      ],
      "text/markdown": [
       "'Spam'"
      ],
      "text/plain": [
       "[1] \"Spam\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prob <- 0.82\n",
    "\n",
    "if(prob>0.5){\n",
    "  clase<-\"Spam\"\n",
    "} else{\n",
    "  clase<-\"Ham\"\n",
    "}\n",
    "\n",
    "clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa119638",
   "metadata": {},
   "source": [
    "## 3) Tipos de datos en el contexto de ML\n",
    "\n",
    "En R, cada columna de un dataset de Machine Learning tiene un tipo de dato. Si el tipo es incorrecto, el modelo no podrÃ¡ entrenar.\n",
    "\n",
    "Los tipos fundamentales son:\n",
    "\n",
    "* **NumÃ©rico** (`numeric`) â†’ Representa **caracterÃ­sticas continuas** (como el precio de una casa o el peso de una persona).\n",
    "* Ejemplo: `25.5`, `1.80`, `-0.004`\n",
    "\n",
    "\n",
    "* **Entero** (`integer`) â†’ Representa **conteos** (como el nÃºmero de habitaciones o clics en un anuncio).\n",
    "* Ejemplo: `3L`, `100L`\n",
    "\n",
    "\n",
    "* **Texto** (`character`) â†’ Representa **categorÃ­as o etiquetas** (como nombres de ciudades o tipos de flores).\n",
    "* Ejemplo: `\"Iris setosa\"`, `\"CrÃ©dito Aprobado\"`\n",
    "\n",
    "\n",
    "* **LÃ³gico** (`logical`) â†’ Representa **indicadores binarios** (Â¿Es un fraude?, Â¿Tiene seguro?).\n",
    "* Ejemplo: `TRUE`, `FALSE`\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Ver el tipo de una variable\n",
    "\n",
    "En ciencia de datos, lo primero que hacemos al cargar un dataset es revisar sus tipos con `class()`:\n",
    "\n",
    "```r\n",
    "class(dataset$target)\n",
    "\n",
    "```\n",
    "\n",
    "Esto es vital para:\n",
    "\n",
    "* **Entender errores:** Por ejemplo, intentar calcular la media de una columna que R cree que es texto.\n",
    "* **Depurar modelos:** Asegurarse de que el modelo de clasificaciÃ³n reciba valores lÃ³gicos o categorÃ­as, no nÃºmeros sueltos.\n",
    "\n",
    "---\n",
    "\n",
    "### El \"choque\" con Python: El decimal por defecto\n",
    "\n",
    "En **Python**, si haces `a = 3`, el sistema suele inferir que es un `int`.\n",
    "En **R**, si haces `a <- 3`, **R lo guarda como `numeric` (decimal)**.\n",
    "\n",
    "> **Â¿Por quÃ© importa en IA?**\n",
    "> R prioriza la precisiÃ³n estadÃ­stica. En modelos complejos (como regresiones lineales), casi todos los cÃ¡lculos requieren decimales, por lo que R prefiere tenerlos listos desde el principio.\n",
    "\n",
    "---\n",
    "\n",
    "### OptimizaciÃ³n: La \"L\" de Large Datasets\n",
    "\n",
    "Para indicar que un nÃºmero es un **entero puro**, aÃ±adimos una **L**:\n",
    "\n",
    "```r\n",
    "id_usuario <- 5420L\n",
    "\n",
    "```\n",
    "\n",
    "En **Big Data** o modelos con millones de filas, usar `integer` en lugar de `numeric` puede reducir el consumo de memoria de tus vectores a la mitad. En IA, Â¡la eficiencia es clave!\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Idea clave:\n",
    "\n",
    "> En R, los nÃºmeros son \"habitantes del mundo decimal\" a menos que les pongas el pasaporte de la **L**.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo completo\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5628d9f0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'numeric'"
      ],
      "text/latex": [
       "'numeric'"
      ],
      "text/markdown": [
       "'numeric'"
      ],
      "text/plain": [
       "[1] \"numeric\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'integer'"
      ],
      "text/latex": [
       "'integer'"
      ],
      "text/markdown": [
       "'integer'"
      ],
      "text/plain": [
       "[1] \"integer\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'character'"
      ],
      "text/latex": [
       "'character'"
      ],
      "text/markdown": [
       "'character'"
      ],
      "text/plain": [
       "[1] \"character\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'logical'"
      ],
      "text/latex": [
       "'logical'"
      ],
      "text/markdown": [
       "'logical'"
      ],
      "text/plain": [
       "[1] \"logical\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a <- 3\n",
    "b <- 3L\n",
    "c <- \"hola\"\n",
    "d <- TRUE\n",
    "\n",
    "class(a)  # numeric\n",
    "class(b)  # integer\n",
    "class(c)  # character\n",
    "class(d)  # logical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86d1a51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "âš ï¸ **Error tÃ­pico**\n",
    "Pensar que `3` y `3L` son exactamente lo mismo.\n",
    "No lo son: tienen el mismo valor, pero **no el mismo tipo**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53173850",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'numeric'"
      ],
      "text/latex": [
       "'numeric'"
      ],
      "text/markdown": [
       "'numeric'"
      ],
      "text/plain": [
       "[1] \"numeric\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'integer'"
      ],
      "text/latex": [
       "'integer'"
      ],
      "text/markdown": [
       "'integer'"
      ],
      "text/plain": [
       "[1] \"integer\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'character'"
      ],
      "text/latex": [
       "'character'"
      ],
      "text/markdown": [
       "'character'"
      ],
      "text/plain": [
       "[1] \"character\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'logical'"
      ],
      "text/latex": [
       "'logical'"
      ],
      "text/markdown": [
       "'logical'"
      ],
      "text/plain": [
       "[1] \"logical\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a <- 3.14\n",
    "b <- 3L\n",
    "c <- \"hola\"\n",
    "d <- TRUE\n",
    "\n",
    "class(a)\n",
    "class(b)\n",
    "class(c)\n",
    "class(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c6766",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ”§ Ejercicio 2: Clasificador de Riesgo (DetecciÃ³n de Fraude)\n",
    "\n",
    "En Machine Learning, los modelos de detecciÃ³n de fraude suelen devolver un \"Score de Riesgo\". Tu tarea es crear un pequeÃ±o motor de decisiÃ³n lÃ³gica.\n",
    "\n",
    "1. Crea una variable llamada `score_riesgo` y asÃ­gnale un valor numÃ©rico entre 0 y 100 (por ejemplo: `72`).\n",
    "2. Crea una variable llamada `es_fraude` que use una comparaciÃ³n lÃ³gica:\n",
    "* SerÃ¡ `TRUE` si el `score_riesgo` es mayor o igual a **80** (umbral de alerta).\n",
    "* SerÃ¡ `FALSE` si es menor de 80.\n",
    "\n",
    "\n",
    "3. Muestra por pantalla el valor de `es_fraude`.\n",
    "4. Comprueba el tipo de dato de `es_fraude` usando la funciÃ³n `class()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fec655c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "FALSE"
      ],
      "text/latex": [
       "FALSE"
      ],
      "text/markdown": [
       "FALSE"
      ],
      "text/plain": [
       "[1] FALSE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'logical'"
      ],
      "text/latex": [
       "'logical'"
      ],
      "text/markdown": [
       "'logical'"
      ],
      "text/plain": [
       "[1] \"logical\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: completa\n",
    "\n",
    "score_riesgo <- 72L\n",
    "\n",
    "es_fraude <- FALSE\n",
    "if(score_riesgo >= 80) {\n",
    "    es_fraude <- TRUE\n",
    "}\n",
    "\n",
    "es_fraude\n",
    "class(es_fraude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ebbca",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Operaciones matemÃ¡ticas: El motor de la IA\n",
    "\n",
    "R es, en esencia, un lenguaje estadÃ­stico. Cualquier algoritmo de Machine Learning (desde una regresiÃ³n hasta una red neuronal) no es mÃ¡s que una **sucesiÃ³n masiva de operaciones matemÃ¡ticas**.\n",
    "\n",
    "Las operaciones bÃ¡sicas son tus herramientas para medir el error de un modelo:\n",
    "\n",
    "* **Suma** â†’ `+`\n",
    "* **Resta** â†’ `-` (Ãštil para calcular el **residuo**: la diferencia entre lo real y lo predicho).\n",
    "* **MultiplicaciÃ³n** â†’ `*` (Usada para aplicar **pesos** a las variables).\n",
    "* **DivisiÃ³n** â†’ `/` (Para normalizar datos o calcular porcentajes de acierto).\n",
    "* **Potencia** â†’ `^` (Fundamental para calcular el **Error CuadrÃ¡tico**, que penaliza los errores grandes).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Ejemplo\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de095f56",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "14"
      ],
      "text/latex": [
       "14"
      ],
      "text/markdown": [
       "14"
      ],
      "text/plain": [
       "[1] 14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "6"
      ],
      "text/latex": [
       "6"
      ],
      "text/markdown": [
       "6"
      ],
      "text/plain": [
       "[1] 6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "40"
      ],
      "text/latex": [
       "40"
      ],
      "text/markdown": [
       "40"
      ],
      "text/plain": [
       "[1] 40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "2.5"
      ],
      "text/latex": [
       "2.5"
      ],
      "text/markdown": [
       "2.5"
      ],
      "text/plain": [
       "[1] 2.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "10000"
      ],
      "text/latex": [
       "10000"
      ],
      "text/markdown": [
       "10000"
      ],
      "text/plain": [
       "[1] 10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x <- 10\n",
    "y <- 4\n",
    "\n",
    "x + y\n",
    "x - y\n",
    "x * y\n",
    "x / y\n",
    "x ^ y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f389b0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Orden de las operaciones (Precedencia)\n",
    "\n",
    "En IA, el orden de los factores **sÃ­ altera el producto**. R sigue el estÃ¡ndar matemÃ¡tico:\n",
    "\n",
    "1. **ParÃ©ntesis** `()` â†’ Â¡Ãšsalos siempre para asegurar que tu fÃ³rmula de ML sea correcta!\n",
    "2. **Potencias** `^`\n",
    "3. **MultiplicaciÃ³n/DivisiÃ³n** `*` `/`\n",
    "4. **Suma/Resta** `+` `-`\n",
    "\n",
    "Ejemplo aplicado (CÃ¡lculo de error):\n",
    "\n",
    "```r\n",
    "real <- 10\n",
    "predicho <- 8\n",
    "\n",
    "# Error al cuadrado: (10 - 8)^2 = 4\n",
    "(real - predicho) ^ 2 \n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ’¡ **Idea clave para IA**\n",
    "No veas estas operaciones como \"aritmÃ©tica de primaria\". En Machine Learning, la **resta** es la base para saber cuÃ¡nto se equivocÃ³ el modelo, y la **potencia** es la herramienta para castigar esos errores. Si dominas esto, entenderÃ¡s cÃ³mo \"aprende\" una IA.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d958ef",
   "metadata": {},
   "source": [
    "### ðŸ”§ Ejercicio 3: Calculando el Error de PredicciÃ³n\n",
    "\n",
    "Imagina que tu modelo de ML predijo que una casa valÃ­a **200,000â‚¬**, pero se vendiÃ³ por **220,000â‚¬**.\n",
    "\n",
    "1. Crea la variable `precio_real <- 220000`.\n",
    "2. Crea la variable `precio_predicho <- 200000`.\n",
    "3. Calcula el **error porcentual** con la siguiente fÃ³rmula:\n",
    "\n",
    "$$Error = \\frac{Precio\\_Real - Precio\\_Predicho}{Precio\\_Real} \\times 100$$\n",
    "\n",
    "4. Muestra el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c81aac9c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "9.09090909090909"
      ],
      "text/latex": [
       "9.09090909090909"
      ],
      "text/markdown": [
       "9.09090909090909"
      ],
      "text/plain": [
       "[1] 9.090909"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: completa\n",
    "\n",
    "precio_real <- 220000\n",
    "precio_predicho <- 200000\n",
    "\n",
    "error <- ((precio_real - precio_predicho) / precio_real) * 100\n",
    "\n",
    "error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d7d74",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Funciones Ãºtiles: `print()` y `paste()`\n",
    "\n",
    "En R usamos **funciones** para realizar tareas concretas. Al trabajar con Machine Learning, las usaremos constantemente para monitorizar el entrenamiento de nuestros modelos y generar reportes de mÃ©tricas.\n",
    "\n",
    "---\n",
    "\n",
    "### `print()`: Ver resultados\n",
    "\n",
    "La funciÃ³n `print()` **muestra un valor u objeto por pantalla**. Aunque en la consola de R basta con escribir el nombre de la variable para ver su contenido, usar `print()` dentro de scripts o bucles de entrenamiento es una mejor prÃ¡ctica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99d62100",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.945\n"
     ]
    }
   ],
   "source": [
    "precision <- 0.945\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8275ee",
   "metadata": {},
   "source": [
    "ðŸ’¡ **Nota de experto:**\n",
    "En R, `print()` en realidad es una funciÃ³n \"genÃ©rica\". Esto significa que si le pasas un modelo de IA entero (como un Bosque Aleatorio), no te mostrarÃ¡ solo un nÃºmero, sino un **resumen tÃ©cnico** de todo el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### `paste()` y `paste0()`: Construir reportes\n",
    "\n",
    "La funciÃ³n `paste()` sirve para **unir textos y variables**. En IA es vital para crear mensajes dinÃ¡micos que nos informen sobre el estado de un experimento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35dc5b83",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Ejecutando el modelo: XGBoost'"
      ],
      "text/latex": [
       "'Ejecutando el modelo: XGBoost'"
      ],
      "text/markdown": [
       "'Ejecutando el modelo: XGBoost'"
      ],
      "text/plain": [
       "[1] \"Ejecutando el modelo: XGBoost\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "algoritmo <- \"XGBoost\"\n",
    "paste(\"Ejecutando el modelo:\", algoritmo)\n",
    "# Resultado: \"Ejecutando el modelo: XGBoost\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5115e6",
   "metadata": {},
   "source": [
    "#### Diferencia de separadores:\n",
    "\n",
    "1. **`paste()`**: AÃ±ade un **espacio** por defecto entre elementos. Puedes cambiarlo con el argumento `sep`.\n",
    "2. **`paste0()`**: Es una versiÃ³n rÃ¡pida que pega todo **sin espacios**. Es la favorita de los Data Scientists para crear nombres de archivos (ej: `modelo_v1.rds`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6837786",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"PrecisiÃ³n: 0.92\"\n",
      "[1] \"resultado_experimento_v2.csv\"\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo con separador personalizado\n",
    "print(paste(\"PrecisiÃ³n\", \"0.92\", sep = \": \")) # \"PrecisiÃ³n: 0.92\"\n",
    "\n",
    "# Ejemplo con paste0 para archivos de datos\n",
    "version <- 2\n",
    "print(paste0(\"resultado_experimento_v\", version, \".csv\")) # \"resultado_experimento_v2.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53b7e4d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ðŸ’¡ **Idea clave para IA**\n",
    "`paste()` es tu herramienta de **comunicaciÃ³n**. Se usa para generar logs automÃ¡ticos: *\"Iniciando entrenamiento en el nodo 1...\"* o *\"Alerta: la pÃ©rdida del modelo es superior a 0.5\"*.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee43de",
   "metadata": {},
   "source": [
    "### ðŸ”§ Ejercicio 4 â€” Reporte de EvaluaciÃ³n de IA\n",
    "\n",
    "Imagina que acabas de validar un modelo de clasificaciÃ³n de imÃ¡genes. Crea las siguientes variables:\n",
    "\n",
    "* `modelo`: El nombre de tu algoritmo (ej: `\"ResNet50\"`).\n",
    "* `precision`: Un valor decimal (ej: `0.88`).\n",
    "* `dataset`: El nombre de tu conjunto de datos (ej: `\"CIFAR-10\"`).\n",
    "\n",
    "Usa la funciÃ³n `paste()` para **mostrar** por pantalla un mensaje con este formato:\n",
    "\n",
    "> *\"El modelo [modelo], entrenado con el dataset [dataset], tiene una precisiÃ³n de [precision].\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cad86ab",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'El modelo ResNet50, entrenado con el dataset CIFAR-10, tiene una precision de 0.88'"
      ],
      "text/latex": [
       "'El modelo ResNet50, entrenado con el dataset CIFAR-10, tiene una precision de 0.88'"
      ],
      "text/markdown": [
       "'El modelo ResNet50, entrenado con el dataset CIFAR-10, tiene una precision de 0.88'"
      ],
      "text/plain": [
       "[1] \"El modelo ResNet50, entrenado con el dataset CIFAR-10, tiene una precision de 0.88\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: completa\n",
    "\n",
    "modelo <- \"ResNet50\"\n",
    "precision <- 0.88\n",
    "dataset <- \"CIFAR-10\"\n",
    "\n",
    "paste(\"El modelo \", modelo, \", entrenado con el dataset \", dataset, \", tiene una precision de \", precision, sep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ca13d",
   "metadata": {},
   "source": [
    "## 7) El sistema de ayuda en R\n",
    "\n",
    "En Machine Learning, es imposible recordar todos los parÃ¡metros de cada algoritmo. R tiene un sistema de ayuda integrado que es incluso mÃ¡s directo que el de Python.\n",
    "\n",
    "La forma mÃ¡s rÃ¡pida de consultar la documentaciÃ³n es escribir **`?` delante del nombre de la funciÃ³n**.\n",
    "\n",
    "```r\n",
    "?sd  # Consultar la desviaciÃ³n estÃ¡ndar\n",
    "\n",
    "```\n",
    "\n",
    "Esto abrirÃ¡ el panel de **Help** con la documentaciÃ³n oficial.\n",
    "\n",
    "---\n",
    "\n",
    "### Â¿CÃ³mo leer la ayuda como un Data Scientist?\n",
    "\n",
    "Cuando abras la ayuda de una funciÃ³n de IA, fÃ­jate siempre en estas secciones clave:\n",
    "\n",
    "1. **Description**: QuÃ© hace realmente el algoritmo.\n",
    "2. **Usage**: CÃ³mo se escribe la funciÃ³n y cuÃ¡les son sus valores por defecto.\n",
    "3. **Arguments**: QuÃ© significan parÃ¡metros como `ntree` (nÃºmero de Ã¡rboles) o `k` (vecinos).\n",
    "4. **Examples**: Fragmentos de cÃ³digo listos para copiar y probar.\n",
    "\n",
    "> **Dato clave:** Muchas funciones de R tienen parÃ¡metros opcionales. Por ejemplo, en ML es comÃºn usar `na.rm = TRUE` para que el modelo no falle si faltan datos (valores `NA`).\n",
    "\n",
    "---\n",
    "\n",
    "### Otras formas de pedir ayuda\n",
    "\n",
    "Si no recuerdas el nombre exacto de la funciÃ³n, puedes usar la bÃºsqueda por palabra clave con doble signo de interrogaciÃ³n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1be43215",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm                   package:stats                    R Documentation\n",
      "\n",
      "_\bF_\bi_\bt_\bt_\bi_\bn_\bg _\bG_\be_\bn_\be_\br_\ba_\bl_\bi_\bz_\be_\bd _\bL_\bi_\bn_\be_\ba_\br _\bM_\bo_\bd_\be_\bl_\bs\n",
      "\n",
      "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
      "\n",
      "     â€˜glmâ€™ is used to fit generalized linear models, specified by\n",
      "     giving a symbolic description of the linear predictor and a\n",
      "     description of the error distribution.\n",
      "\n",
      "_\bU_\bs_\ba_\bg_\be:\n",
      "\n",
      "     glm(formula, family = gaussian, data, weights, subset,\n",
      "         na.action, start = NULL, etastart, mustart, offset,\n",
      "         control = list(...), model = TRUE, method = \"glm.fit\",\n",
      "         x = FALSE, y = TRUE, singular.ok = TRUE, contrasts = NULL, ...)\n",
      "     \n",
      "     glm.fit(x, y, weights = rep.int(1, nobs),\n",
      "             start = NULL, etastart = NULL, mustart = NULL,\n",
      "             offset = rep.int(0, nobs), family = gaussian(),\n",
      "             control = list(), intercept = TRUE, singular.ok = TRUE)\n",
      "     \n",
      "     ## S3 method for class 'glm'\n",
      "     weights(object, type = c(\"prior\", \"working\"), ...)\n",
      "     \n",
      "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
      "\n",
      " formula: an object of class â€˜\"formula\"â€™ (or one that can be coerced to\n",
      "          that class): a symbolic description of the model to be\n",
      "          fitted.  The details of model specification are given under\n",
      "          â€˜Detailsâ€™.\n",
      "\n",
      "  family: a description of the error distribution and link function to\n",
      "          be used in the model.  For â€˜glmâ€™ this can be a character\n",
      "          string naming a family function, a family function or the\n",
      "          result of a call to a family function.  For â€˜glm.fitâ€™ only\n",
      "          the third option is supported.  (See â€˜familyâ€™ for details of\n",
      "          family functions.)\n",
      "\n",
      "    data: an optional data frame, list or environment (or object\n",
      "          coercible by â€˜as.data.frameâ€™ to a data frame) containing the\n",
      "          variables in the model.  If not found in â€˜dataâ€™, the\n",
      "          variables are taken from â€˜environment(formula)â€™, typically\n",
      "          the environment from which â€˜glmâ€™ is called.\n",
      "\n",
      " weights: an optional vector of â€˜prior weightsâ€™ to be used in the\n",
      "          fitting process.  Should be â€˜NULLâ€™ or a numeric vector.\n",
      "\n",
      "  subset: an optional vector specifying a subset of observations to be\n",
      "          used in the fitting process.  (See additional details about\n",
      "          how this argument interacts with data-dependent bases in the\n",
      "          â€˜Detailsâ€™ below.)\n",
      "\n",
      "na.action: a function which indicates what should happen when the data\n",
      "          contain â€˜NAâ€™s.  The default is set by the â€˜na.actionâ€™ setting\n",
      "          of â€˜optionsâ€™, and is â€˜na.failâ€™ if that is unset.  The\n",
      "          â€˜factory-freshâ€™ default is â€˜na.omitâ€™.  Another possible value\n",
      "          is â€˜NULLâ€™, no action.  Value â€˜na.excludeâ€™ can be useful.\n",
      "\n",
      "   start: starting values for the parameters in the linear predictor.\n",
      "\n",
      "etastart: starting values for the linear predictor.\n",
      "\n",
      " mustart: starting values for the vector of means.\n",
      "\n",
      "  offset: this can be used to specify an _a priori_ known component to\n",
      "          be included in the linear predictor during fitting.  This\n",
      "          should be â€˜NULLâ€™ or a numeric vector of length equal to the\n",
      "          number of cases.  One or more â€˜offsetâ€™ terms can be included\n",
      "          in the formula instead or as well, and if more than one is\n",
      "          specified their sum is used.  See â€˜model.offsetâ€™.\n",
      "\n",
      " control: a list of parameters for controlling the fitting process.\n",
      "          For â€˜glm.fitâ€™ this is passed to â€˜glm.controlâ€™.\n",
      "\n",
      "   model: a logical value indicating whether _model frame_ should be\n",
      "          included as a component of the returned value.\n",
      "\n",
      "  method: the method to be used in fitting the model.  The default\n",
      "          method â€˜\"glm.fit\"â€™ uses iteratively reweighted least squares\n",
      "          (IWLS): the alternative â€˜\"model.frame\"â€™ returns the model\n",
      "          frame and does no fitting.\n",
      "\n",
      "          User-supplied fitting functions can be supplied either as a\n",
      "          function or a character string naming a function, with a\n",
      "          function which takes the same arguments as â€˜glm.fitâ€™.  If\n",
      "          specified as a character string it is looked up from within\n",
      "          the â€˜statsâ€™ namespace.\n",
      "\n",
      "    x, y: For â€˜glmâ€™: logical values indicating whether the response\n",
      "          vector and model matrix used in the fitting process should be\n",
      "          returned as components of the returned value.\n",
      "\n",
      "          For â€˜glm.fitâ€™: â€˜xâ€™ is a design matrix of dimension â€˜n * pâ€™,\n",
      "          and â€˜yâ€™ is a vector of observations of length â€˜nâ€™.\n",
      "\n",
      "singular.ok: logical; if â€˜FALSEâ€™ a singular fit is an error.\n",
      "\n",
      "contrasts: an optional list. See the â€˜contrasts.argâ€™ of\n",
      "          â€˜model.matrix.defaultâ€™.\n",
      "\n",
      "intercept: logical. Should an intercept be included in the _null_\n",
      "          model?\n",
      "\n",
      "  object: an object inheriting from class â€˜\"glm\"â€™.\n",
      "\n",
      "    type: character, partial matching allowed.  Type of weights to\n",
      "          extract from the fitted model object.  Can be abbreviated.\n",
      "\n",
      "     ...: For â€˜glmâ€™: arguments to be used to form the default â€˜controlâ€™\n",
      "          argument if it is not supplied directly.\n",
      "\n",
      "          For â€˜weightsâ€™: further arguments passed to or from other\n",
      "          methods.\n",
      "\n",
      "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
      "\n",
      "     A typical predictor has the form â€˜response ~ termsâ€™ where\n",
      "     â€˜responseâ€™ is the (numeric) response vector and â€˜termsâ€™ is a\n",
      "     series of terms which specifies a linear predictor for â€˜responseâ€™.\n",
      "     For â€˜binomialâ€™ and â€˜quasibinomialâ€™ families the response can also\n",
      "     be specified as a â€˜factorâ€™ (when the first level denotes failure\n",
      "     and all others success) or as a two-column matrix with the columns\n",
      "     giving the numbers of successes and failures.  A terms\n",
      "     specification of the form â€˜first + secondâ€™ indicates all the terms\n",
      "     in â€˜firstâ€™ together with all the terms in â€˜secondâ€™ with any\n",
      "     duplicates removed.\n",
      "\n",
      "     A specification of the form â€˜first:secondâ€™ indicates the set of\n",
      "     terms obtained by taking the interactions of all terms in â€˜firstâ€™\n",
      "     with all terms in â€˜secondâ€™.  The specification â€˜first*secondâ€™\n",
      "     indicates the _cross_ of â€˜firstâ€™ and â€˜secondâ€™.  This is the same\n",
      "     as â€˜first + second + first:secondâ€™.\n",
      "\n",
      "     The terms in the formula will be re-ordered so that main effects\n",
      "     come first, followed by the interactions, all second-order, all\n",
      "     third-order and so on: to avoid this pass a â€˜termsâ€™ object as the\n",
      "     formula.\n",
      "\n",
      "     Non-â€˜NULLâ€™ â€˜weightsâ€™ can be used to indicate that different\n",
      "     observations have different dispersions (with the values in\n",
      "     â€˜weightsâ€™ being inversely proportional to the dispersions); or\n",
      "     equivalently, when the elements of â€˜weightsâ€™ are positive integers\n",
      "     w_i, that each response y_i is the mean of w_i unit-weight\n",
      "     observations.  For a binomial GLM prior weights are used to give\n",
      "     the number of trials when the response is the proportion of\n",
      "     successes: they would rarely be used for a Poisson GLM.\n",
      "\n",
      "     â€˜glm.fitâ€™ is the workhorse function: it is not normally called\n",
      "     directly but can be more efficient where the response vector,\n",
      "     design matrix and family have already been calculated.\n",
      "\n",
      "     If more than one of â€˜etastartâ€™, â€˜startâ€™ and â€˜mustartâ€™ is\n",
      "     specified, the first in the list will be used.  It is often\n",
      "     advisable to supply starting values for a â€˜quasiâ€™ family, and also\n",
      "     for families with unusual links such as â€˜gaussian(\"log\")â€™.\n",
      "\n",
      "     All of â€˜weightsâ€™, â€˜subsetâ€™, â€˜offsetâ€™, â€˜etastartâ€™ and â€˜mustartâ€™ are\n",
      "     evaluated in the same way as variables in â€˜formulaâ€™, that is first\n",
      "     in â€˜dataâ€™ and then in the environment of â€˜formulaâ€™.  Note that\n",
      "     values calculated inside the formula, such as â€˜mean(x)â€™, are\n",
      "     evaluated before subsetting - which may lead to unexpected results\n",
      "     if used with â€˜subsetâ€™.  For more information see the â€˜Detailsâ€™\n",
      "     section of the â€˜model.frameâ€™.\n",
      "\n",
      "     For the background to warning messages about â€˜fitted probabilities\n",
      "     numerically 0 or 1 occurredâ€™ for binomial GLMs, see Venables &\n",
      "     Ripley (2002, pp. 197-8).\n",
      "\n",
      "_\bV_\ba_\bl_\bu_\be:\n",
      "\n",
      "     â€˜glmâ€™ returns an object of class inheriting from â€˜\"glm\"â€™ which\n",
      "     inherits from the class â€˜\"lm\"â€™. See later in this section.  If a\n",
      "     non-standard â€˜methodâ€™ is used, the object will also inherit from\n",
      "     the class (if any) returned by that function.\n",
      "\n",
      "     The function â€˜summaryâ€™ (i.e., â€˜summary.glmâ€™) can be used to obtain\n",
      "     or print a summary of the results and the function â€˜anovaâ€™ (i.e.,\n",
      "     â€˜anova.glmâ€™) to produce an analysis of variance table.\n",
      "\n",
      "     The generic accessor functions â€˜coefficientsâ€™, â€˜effectsâ€™,\n",
      "     â€˜fitted.valuesâ€™ and â€˜residualsâ€™ can be used to extract various\n",
      "     useful features of the value returned by â€˜glmâ€™.\n",
      "\n",
      "     â€˜weightsâ€™ extracts a vector of weights, one for each case in the\n",
      "     fit (after subsetting and â€˜na.actionâ€™).\n",
      "\n",
      "     An object of class â€˜\"glm\"â€™ is a list containing at least the\n",
      "     following components:\n",
      "\n",
      "coefficients: a named vector of coefficients\n",
      "\n",
      "residuals: the _working_ residuals, that is the residuals in the final\n",
      "          iteration of the IWLS fit.  Since cases with zero weights are\n",
      "          omitted, their working residuals are â€˜NAâ€™.\n",
      "\n",
      "fitted.values: the fitted mean values, obtained by transforming the\n",
      "          linear predictors by the inverse of the link function.\n",
      "\n",
      "    rank: the numeric rank of the fitted linear model.\n",
      "\n",
      "  family: the â€˜familyâ€™ object used.\n",
      "\n",
      "linear.predictors: the linear fit on link scale.\n",
      "\n",
      "deviance: up to a constant, minus twice the maximized log-likelihood.\n",
      "          Where sensible, the constant is chosen so that a saturated\n",
      "          model has deviance zero.\n",
      "\n",
      "     aic: A version of Akaike's _An Information Criterion_, minus twice\n",
      "          the maximized log-likelihood plus twice the number of\n",
      "          parameters, computed via the â€˜aicâ€™ component of the family.\n",
      "          For binomial and Poison families the dispersion is fixed at\n",
      "          one and the number of parameters is the number of\n",
      "          coefficients.  For gaussian, Gamma and inverse gaussian\n",
      "          families the dispersion is estimated from the residual\n",
      "          deviance, and the number of parameters is the number of\n",
      "          coefficients plus one.  For a gaussian family the MLE of the\n",
      "          dispersion is used so this is a valid value of AIC, but for\n",
      "          Gamma and inverse gaussian families it is not.  For families\n",
      "          fitted by quasi-likelihood the value is â€˜NAâ€™.\n",
      "\n",
      "null.deviance: The deviance for the null model, comparable with\n",
      "          â€˜devianceâ€™. The null model will include the offset, and an\n",
      "          intercept if there is one in the model.  Note that this will\n",
      "          be incorrect if the link function depends on the data other\n",
      "          than through the fitted mean: specify a zero offset to force\n",
      "          a correct calculation.\n",
      "\n",
      "    iter: the number of iterations of IWLS used.\n",
      "\n",
      " weights: the _working_ weights, that is the weights in the final\n",
      "          iteration of the IWLS fit.\n",
      "\n",
      "prior.weights: the weights initially supplied, a vector of â€˜1â€™s if none\n",
      "          were.\n",
      "\n",
      "df.residual: the residual degrees of freedom.\n",
      "\n",
      " df.null: the residual degrees of freedom for the null model.\n",
      "\n",
      "       y: if requested (the default) the â€˜yâ€™ vector used. (It is a\n",
      "          vector even for a binomial model.)\n",
      "\n",
      "       x: if requested, the model matrix.\n",
      "\n",
      "   model: if requested (the default), the model frame.\n",
      "\n",
      "converged: logical. Was the IWLS algorithm judged to have converged?\n",
      "\n",
      "boundary: logical. Is the fitted value on the boundary of the\n",
      "          attainable values?\n",
      "\n",
      "    call: the matched call.\n",
      "\n",
      " formula: the formula supplied.\n",
      "\n",
      "   terms: the â€˜termsâ€™ object used.\n",
      "\n",
      "    data: the â€˜data argumentâ€™.\n",
      "\n",
      "  offset: the offset vector used.\n",
      "\n",
      " control: the value of the â€˜controlâ€™ argument used.\n",
      "\n",
      "  method: the name of the fitter function used (when provided as a\n",
      "          â€˜characterâ€™ string to â€˜glm()â€™) or the fitter â€˜functionâ€™ (when\n",
      "          provided as that).\n",
      "\n",
      "contrasts: (where relevant) the contrasts used.\n",
      "\n",
      " xlevels: (where relevant) a record of the levels of the factors used\n",
      "          in fitting.\n",
      "\n",
      "na.action: (where relevant) information returned by â€˜model.frameâ€™ on\n",
      "          the special handling of â€˜NAâ€™s.\n",
      "\n",
      "     In addition, non-empty fits will have components â€˜qrâ€™, â€˜Râ€™ and\n",
      "     â€˜effectsâ€™ relating to the final weighted linear fit.\n",
      "\n",
      "     Objects of class â€˜\"glm\"â€™ are normally of class â€˜c(\"glm\", \"lm\")â€™,\n",
      "     that is inherit from class â€˜\"lm\"â€™, and well-designed methods for\n",
      "     class â€˜\"lm\"â€™ will be applied to the weighted linear model at the\n",
      "     final iteration of IWLS.  However, care is needed, as extractor\n",
      "     functions for class â€˜\"glm\"â€™ such as â€˜residualsâ€™ and â€˜weightsâ€™ do\n",
      "     *not* just pick out the component of the fit with the same name.\n",
      "\n",
      "     If a â€˜binomialâ€™ â€˜glmâ€™ model was specified by giving a two-column\n",
      "     response, the weights returned by â€˜prior.weightsâ€™ are the total\n",
      "     numbers of cases (factored by the supplied case weights) and the\n",
      "     component â€˜yâ€™ of the result is the proportion of successes.\n",
      "\n",
      "_\bF_\bi_\bt_\bt_\bi_\bn_\bg _\bf_\bu_\bn_\bc_\bt_\bi_\bo_\bn_\bs:\n",
      "\n",
      "     The argument â€˜methodâ€™ serves two purposes.  One is to allow the\n",
      "     model frame to be recreated with no fitting.  The other is to\n",
      "     allow the default fitting function â€˜glm.fitâ€™ to be replaced by a\n",
      "     function which takes the same arguments and uses a different\n",
      "     fitting algorithm.  If â€˜glm.fitâ€™ is supplied as a character string\n",
      "     it is used to search for a function of that name, starting in the\n",
      "     â€˜statsâ€™ namespace.\n",
      "\n",
      "     The class of the object return by the fitter (if any) will be\n",
      "     prepended to the class returned by â€˜glmâ€™.\n",
      "\n",
      "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
      "\n",
      "     The original R implementation of â€˜glmâ€™ was written by Simon Davies\n",
      "     working for Ross Ihaka at the University of Auckland, but has\n",
      "     since been extensively re-written by members of the R Core team.\n",
      "\n",
      "     The design was inspired by the S function of the same name\n",
      "     described in Hastie & Pregibon (1992).\n",
      "\n",
      "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
      "\n",
      "     Dobson, A. J. (1990) _An Introduction to Generalized Linear\n",
      "     Models._ London: Chapman and Hall.\n",
      "\n",
      "     Hastie, T. J. and Pregibon, D. (1992) _Generalized linear models._\n",
      "     Chapter 6 of _Statistical Models in S_ eds J. M. Chambers and T.\n",
      "     J. Hastie, Wadsworth & Brooks/Cole.\n",
      "\n",
      "     McCullagh P. and Nelder, J. A. (1989) _Generalized Linear Models._\n",
      "     London: Chapman and Hall.\n",
      "\n",
      "     Venables, W. N. and Ripley, B. D. (2002) _Modern Applied\n",
      "     Statistics with S._ New York: Springer.\n",
      "\n",
      "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
      "\n",
      "     â€˜anova.glmâ€™, â€˜summary.glmâ€™, etc. for â€˜glmâ€™ methods, and the\n",
      "     generic functions â€˜anovaâ€™, â€˜summaryâ€™, â€˜effectsâ€™, â€˜fitted.valuesâ€™,\n",
      "     and â€˜residualsâ€™.\n",
      "\n",
      "     â€˜lmâ€™ for non-generalized _linear_ models (which SAS calls GLMs,\n",
      "     for â€˜generalâ€™ linear models).\n",
      "\n",
      "     â€˜loglinâ€™ and â€˜loglmâ€™ (package â€˜MASSâ€™) for fitting log-linear\n",
      "     models (which binomial and Poisson GLMs are) to contingency\n",
      "     tables.\n",
      "\n",
      "     â€˜bigglmâ€™ in package â€˜biglmâ€™ for an alternative way to fit GLMs to\n",
      "     large datasets (especially those with many cases).\n",
      "\n",
      "     â€˜esophâ€™, â€˜infertâ€™ and â€˜predict.glmâ€™ have examples of fitting\n",
      "     binomial GLMs.\n",
      "\n",
      "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
      "\n",
      "     ## Dobson (1990) Page 93: Randomized Controlled Trial :\n",
      "     counts <- c(18,17,15,20,10,20,25,13,12)\n",
      "     outcome <- gl(3,1,9)\n",
      "     treatment <- gl(3,3)\n",
      "     data.frame(treatment, outcome, counts) # showing data\n",
      "     glm.D93 <- glm(counts ~ outcome + treatment, family = poisson())\n",
      "     anova(glm.D93)\n",
      "     summary(glm.D93)\n",
      "     ## Computing AIC [in many ways]:\n",
      "     (A0 <- AIC(glm.D93))\n",
      "     (ll <- logLik(glm.D93))\n",
      "     A1 <- -2*c(ll) + 2*attr(ll, \"df\")\n",
      "     A2 <- glm.D93$family$aic(counts, mu=fitted(glm.D93), wt=1) +\n",
      "             2 * length(coef(glm.D93))\n",
      "     stopifnot(exprs = {\n",
      "       all.equal(A0, A1)\n",
      "       all.equal(A1, A2)\n",
      "       all.equal(A1, glm.D93$aic)\n",
      "     })\n",
      "     \n",
      "     \n",
      "     ## an example with offsets from Venables & Ripley (2002, p.189)\n",
      "     utils::data(anorexia, package = \"MASS\")\n",
      "     \n",
      "     anorex.1 <- glm(Postwt ~ Prewt + Treat + offset(Prewt),\n",
      "                     family = gaussian, data = anorexia)\n",
      "     summary(anorex.1)\n",
      "     \n",
      "     \n",
      "     # A Gamma example, from McCullagh & Nelder (1989, pp. 300-2)\n",
      "     clotting <- data.frame(\n",
      "         u = c(5,10,15,20,30,40,60,80,100),\n",
      "         lot1 = c(118,58,42,35,27,25,21,19,18),\n",
      "         lot2 = c(69,35,26,21,18,16,13,12,12))\n",
      "     summary(glm(lot1 ~ log(u), data = clotting, family = Gamma))\n",
      "     summary(glm(lot2 ~ log(u), data = clotting, family = Gamma))\n",
      "     ## Aliased (\"S\"ingular) -> 1 NA coefficient\n",
      "     (fS <- glm(lot2 ~ log(u) + log(u^2), data = clotting, family = Gamma))\n",
      "     tools::assertError(update(fS, singular.ok=FALSE), verbose=interactive())\n",
      "     ## -> .. \"singular fit encountered\"\n",
      "     \n",
      "     ## Not run:\n",
      "     \n",
      "     ## for an example of the use of a terms object as a formula\n",
      "     demo(glm.vr)\n",
      "     ## End(Not run)\n",
      "     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Information\n",
      "\n",
      "Vignettes with name or keyword or title matching â€˜regressionâ€™ using\n",
      "fuzzy matching:\n",
      "\n",
      "\n",
      "e1071::svmdoc           Support Vector Machines---the Interface to\n",
      "                        libsvm in package e1071\n",
      "  Concepts: regression\n",
      "e1071::svminternals     svm() internals\n",
      "  Concepts: regression\n",
      "lmtest::`lmtest-intro`\n",
      "                        Diagnostic Checking in Regression Relationships\n",
      "\n",
      "\n",
      "Type 'vignette(PKG::FOO)' to inspect entries 'PKG::FOO'.\n",
      "\n",
      "\n",
      "\n",
      "Demos with name or title matching â€˜regressionâ€™ using fuzzy matching:\n",
      "\n",
      "\n",
      "SQUAREM::mmlogistic     Examples of MM acceleration for Logistics\n",
      "                        Regression Maximum Likelihood Estimation.\n",
      "\n",
      "\n",
      "Type 'demo(PKG::FOO)' to run demonstration 'PKG::FOO'.\n",
      "\n",
      "\n",
      "\n",
      "Help files with alias or concept or title matching â€˜regressionâ€™ using\n",
      "fuzzy matching:\n",
      "\n",
      "\n",
      "broom::tidy.multinom    Tidying methods for multinomial logistic\n",
      "                        regression models\n",
      "caret::bagEarth         Bagged Earth\n",
      "  Concepts: Regression\n",
      "caret::bagFDA           Bagged FDA\n",
      "  Concepts: Regression\n",
      "caret::icr.formula      Independent Component Regression\n",
      "caret::knnreg           k-Nearest Neighbour Regression\n",
      "caret::plotObsVsPred    Plot Observed versus Predicted Results in\n",
      "                        Regression and Classification Models\n",
      "caret::predict.bagEarth\n",
      "                        Predicted values based on bagged Earth and FDA\n",
      "                        models\n",
      "  Concepts: Regression\n",
      "caret::predict.knnreg   Predictions from k-Nearest Neighbors Regression\n",
      "                        Model\n",
      "caret::varImp           Calculation of variable importance for\n",
      "                        regression and classification models\n",
      "datasets::anscombe      Anscombe's Quartet of 'Identical' Simple Linear\n",
      "                        Regressions\n",
      "datasets::longley       Longley's Economic Regression Data\n",
      "dials::`bart-param`     Parameters for BART models These parameters are\n",
      "                        used for constructing Bayesian adaptive\n",
      "                        regression tree (BART) models.\n",
      "e1071::gknn             Generalized k-Nearest Neighbors Classification\n",
      "                        or Regression\n",
      "  Concepts: Non-linear Regression\n",
      "e1071::plot.svm         Plot SVM Objects\n",
      "  Concepts: Non-linear Regression\n",
      "e1071::predict.svm      Predict Method for Support Vector Machines\n",
      "  Concepts: Non-linear Regression\n",
      "e1071::svm              Support Vector Machines\n",
      "  Concepts: Non-linear Regression\n",
      "e1071::write.svm        Write SVM Object to File\n",
      "  Concepts: Non-linear Regression\n",
      "forecast::arima.errors\n",
      "                        Errors from a regression model with ARIMA\n",
      "                        errors\n",
      "ggplot2::geom_quantile\n",
      "                        Quantile regression\n",
      "ipred::bagging          Bagging Classification, Regression and Survival\n",
      "                        Trees\n",
      "  Concepts: Regression and Classification Trees\n",
      "ipred::predict.classbagg\n",
      "                        Predictions from Bagging Trees\n",
      "  Concepts: Regression and Classification Trees\n",
      "ipred::print.classbagg\n",
      "                        Print Method for Bagging Trees\n",
      "  Concepts: Regression and Classification Trees\n",
      "ipred::prune.classbagg\n",
      "                        Pruning for Bagging\n",
      "  Concepts: Regression and Classification Trees\n",
      "ipred::summary.classbagg\n",
      "                        Summarising Bagging\n",
      "  Concepts: Regression and Classification Trees\n",
      "KernSmooth::dpill       Select a Bandwidth for Local Linear Regression\n",
      "KernSmooth::locpoly     Estimate Functions Using Local Polynomials\n",
      "  Concepts: Regression\n",
      "lava::`constrain<-`     Add non-linear constraints to latent variable\n",
      "                        model\n",
      "  Concepts: Regression\n",
      "lava::`regression<-`    Add regression association to latent variable\n",
      "                        model\n",
      "  Aliases: regression<-, regression, regression<-.lvm, regression.lvm\n",
      "  Concepts: Regression\n",
      "lava::bootstrap.lvm     Calculate bootstrap estimates of a lvm object\n",
      "  Concepts: Regression\n",
      "lava::complik           Composite Likelihood for probit latent variable\n",
      "                        models\n",
      "  Concepts: Regression\n",
      "lava::confint.lvmfit    Calculate confidence limits for parameters\n",
      "  Concepts: Regression\n",
      "lava::covariance        Add covariance structure to Latent Variable\n",
      "                        Model\n",
      "  Concepts: Regression\n",
      "lava::estimate.lvm      Estimation of parameters in a Latent Variable\n",
      "                        Model (lvm)\n",
      "  Concepts: Regression\n",
      "lava::eventTime         Add an observed event time outcome to a latent\n",
      "                        variable model.\n",
      "  Concepts: Regression\n",
      "lava::intercept         Fix mean parameters in 'lvm'-object\n",
      "  Concepts: Regression\n",
      "lava::lvm               Initialize new latent variable model\n",
      "  Concepts: Regression\n",
      "lava::mixture           Estimate mixture latent variable model.\n",
      "  Concepts: Regression\n",
      "lava::mvnmix            Estimate mixture latent variable model\n",
      "  Concepts: Regression\n",
      "lava::ordreg            Univariate cumulative link regression models\n",
      "lava::partialcor        Calculate partial correlations\n",
      "  Concepts: Regression\n",
      "lava::PD                Dose response calculation for binomial\n",
      "                        regression models\n",
      "lava::plot.lvm          Plot path diagram\n",
      "  Concepts: Regression\n",
      "lava::plotConf          Plot regression lines\n",
      "  Concepts: Regression\n",
      "lava::rmvar             Remove variables from (model) object.\n",
      "  Concepts: Regression\n",
      "lava::sim.lvm           Simulate model\n",
      "  Concepts: Regression\n",
      "lava::subset.lvm        Extract subset of latent variable model\n",
      "  Concepts: Regression\n",
      "lava::vars              Extract variable names from latent variable\n",
      "                        model\n",
      "  Concepts: Regression\n",
      "lava::zibreg            Regression model for binomial data with unkown\n",
      "                        group of immortals\n",
      "MASS::anova.negbin      Likelihood Ratio Tests for Negative Binomial\n",
      "                        GLMs\n",
      "  Concepts: Regression\n",
      "MASS::area              Adaptive Numerical Integration\n",
      "  Concepts: Non-linear Regression\n",
      "MASS::boxcox            Box-Cox Transformations for Linear Models\n",
      "  Concepts: Regression\n",
      "MASS::dose.p            Predict Doses for Binomial Assay model\n",
      "  Concepts: Regression\n",
      "MASS::glm.convert       Change a Negative Binomial fit to a GLM fit\n",
      "  Concepts: Regression\n",
      "MASS::glm.nb            Fit a Negative Binomial Generalized Linear\n",
      "                        Model\n",
      "  Concepts: Regression\n",
      "MASS::lm.ridge          Ridge Regression\n",
      "MASS::logtrans          Estimate log Transformation Parameter\n",
      "  Concepts: Regression\n",
      "MASS::lqs               Resistant Regression\n",
      "MASS::negative.binomial\n",
      "                        Family function for Negative Binomial GLMs\n",
      "  Concepts: Regression\n",
      "MASS::polr              Ordered Logistic or Probit Regression\n",
      "MASS::profile.glm       Method for Profiling glm Objects\n",
      "  Concepts: Regression\n",
      "MASS::rms.curv          Relative Curvature Measures for Non-Linear\n",
      "                        Regression\n",
      "  Concepts: Non-linear Regression\n",
      "modeldata::sim_classification\n",
      "                        Simulate datasets\n",
      "  Aliases: sim_regression\n",
      "parsnip::bart           Bayesian additive regression trees (BART)\n",
      "parsnip::cubist_rules   Cubist rule-based regression models\n",
      "parsnip::details_bart_dbarts\n",
      "                        Bayesian additive regression trees via dbarts\n",
      "parsnip::details_cubist_rules_Cubist\n",
      "                        Cubist rule-based regression models\n",
      "parsnip::details_linear_reg_brulee\n",
      "                        Linear regression via brulee\n",
      "parsnip::details_linear_reg_gee\n",
      "                        Linear regression via generalized estimating\n",
      "                        equations (GEE)\n",
      "parsnip::details_linear_reg_glm\n",
      "                        Linear regression via glm\n",
      "parsnip::details_linear_reg_glmer\n",
      "                        Linear regression via generalized mixed models\n",
      "parsnip::details_linear_reg_glmnet\n",
      "                        Linear regression via glmnet\n",
      "parsnip::details_linear_reg_gls\n",
      "                        Linear regression via generalized least squares\n",
      "parsnip::details_linear_reg_h2o\n",
      "                        Linear regression via h2o\n",
      "parsnip::details_linear_reg_keras\n",
      "                        Linear regression via keras/tensorflow\n",
      "parsnip::details_linear_reg_lm\n",
      "                        Linear regression via lm\n",
      "parsnip::details_linear_reg_lme\n",
      "                        Linear regression via mixed models\n",
      "parsnip::details_linear_reg_lmer\n",
      "                        Linear regression via mixed models\n",
      "parsnip::details_linear_reg_quantreg\n",
      "                        Linear quantile regression via the quantreg\n",
      "                        package\n",
      "parsnip::details_linear_reg_spark\n",
      "                        Linear regression via spark\n",
      "parsnip::details_linear_reg_stan\n",
      "                        Linear regression via Bayesian Methods\n",
      "parsnip::details_linear_reg_stan_glmer\n",
      "                        Linear regression via hierarchical Bayesian\n",
      "                        methods\n",
      "parsnip::details_logistic_reg_brulee\n",
      "                        Logistic regression via brulee\n",
      "parsnip::details_logistic_reg_gee\n",
      "                        Logistic regression via generalized estimating\n",
      "                        equations (GEE)\n",
      "parsnip::details_logistic_reg_glm\n",
      "                        Logistic regression via glm\n",
      "parsnip::details_logistic_reg_glmer\n",
      "                        Logistic regression via mixed models\n",
      "parsnip::details_logistic_reg_glmnet\n",
      "                        Logistic regression via glmnet\n",
      "parsnip::details_logistic_reg_h2o\n",
      "                        Logistic regression via h2o\n",
      "parsnip::details_logistic_reg_keras\n",
      "                        Logistic regression via keras\n",
      "parsnip::details_logistic_reg_LiblineaR\n",
      "                        Logistic regression via LiblineaR\n",
      "parsnip::details_logistic_reg_spark\n",
      "                        Logistic regression via spark\n",
      "parsnip::details_logistic_reg_stan\n",
      "                        Logistic regression via stan\n",
      "parsnip::details_logistic_reg_stan_glmer\n",
      "                        Logistic regression via hierarchical Bayesian\n",
      "                        methods\n",
      "parsnip::details_mars_earth\n",
      "                        Multivariate adaptive regression splines (MARS)\n",
      "                        via earth\n",
      "parsnip::details_multinom_reg_brulee\n",
      "                        Multinomial regression via brulee\n",
      "parsnip::details_multinom_reg_glmnet\n",
      "                        Multinomial regression via glmnet\n",
      "parsnip::details_multinom_reg_h2o\n",
      "                        Multinomial regression via h2o\n",
      "parsnip::details_multinom_reg_keras\n",
      "                        Multinomial regression via keras\n",
      "parsnip::details_multinom_reg_nnet\n",
      "                        Multinomial regression via nnet\n",
      "parsnip::details_multinom_reg_spark\n",
      "                        Multinomial regression via spark\n",
      "parsnip::details_poisson_reg_gee\n",
      "                        Poisson regression via generalized estimating\n",
      "                        equations (GEE)\n",
      "parsnip::details_poisson_reg_glm\n",
      "                        Poisson regression via glm\n",
      "parsnip::details_poisson_reg_glmer\n",
      "                        Poisson regression via mixed models\n",
      "parsnip::details_poisson_reg_glmnet\n",
      "                        Poisson regression via glmnet\n",
      "parsnip::details_poisson_reg_h2o\n",
      "                        Poisson regression via h2o\n",
      "parsnip::details_poisson_reg_hurdle\n",
      "                        Poisson regression via pscl\n",
      "parsnip::details_poisson_reg_stan\n",
      "                        Poisson regression via stan\n",
      "parsnip::details_poisson_reg_stan_glmer\n",
      "                        Poisson regression via hierarchical Bayesian\n",
      "                        methods\n",
      "parsnip::details_poisson_reg_zeroinfl\n",
      "                        Poisson regression via pscl\n",
      "parsnip::details_proportional_hazards_glmnet\n",
      "                        Proportional hazards regression\n",
      "parsnip::details_proportional_hazards_survival\n",
      "                        Proportional hazards regression\n",
      "parsnip::details_survival_reg_flexsurv\n",
      "                        Parametric survival regression\n",
      "parsnip::details_survival_reg_flexsurvspline\n",
      "                        Flexible parametric survival regression\n",
      "parsnip::details_survival_reg_survival\n",
      "                        Parametric survival regression\n",
      "parsnip::linear_reg     Linear regression\n",
      "parsnip::logistic_reg   Logistic regression\n",
      "parsnip::mars           Multivariate adaptive regression splines (MARS)\n",
      "parsnip::multinom_reg   Multinomial regression\n",
      "parsnip::poisson_reg    Poisson regression models\n",
      "parsnip::proportional_hazards\n",
      "                        Proportional hazards regression\n",
      "parsnip::surv_reg       Parametric survival regression\n",
      "parsnip::survival_reg   Parametric survival regression\n",
      "progressr::global_progression_handler\n",
      "                        A Global Calling Handler For 'progression':s\n",
      "  Aliases: global_progression_handler\n",
      "progressr::handler_ascii_alert\n",
      "                        Progression Handler: Progress Reported as ASCII\n",
      "                        BEL Symbols (Audio or Blink) in the Terminal\n",
      "progressr::handler_beepr\n",
      "                        Progression Handler: Progress Reported as\n",
      "                        'beepr' Sounds (Audio)\n",
      "progressr::handler_cli\n",
      "                        Progression Handler: Progress Reported via\n",
      "                        'cli' Progress Bars (Text) in the Terminal\n",
      "progressr::handler_debug\n",
      "                        Progression Handler: Progress Reported as Debug\n",
      "                        Information (Text) in the Terminal\n",
      "progressr::handler_filesize\n",
      "                        Progression Handler: Progress Reported as the\n",
      "                        Size of a File on the File System\n",
      "progressr::handler_newline\n",
      "                        Progression Handler: Progress Reported as a New\n",
      "                        Line (Text) in the Terminal\n",
      "progressr::handler_notifier\n",
      "                        Progression Handler: Progress Reported via the\n",
      "                        Operating-System Notification Framework (GUI,\n",
      "                        Text)\n",
      "progressr::handler_ntfy\n",
      "                        Progression Handler: Progress Reported via the\n",
      "                        Ntfy.sh Messaging Service\n",
      "progressr::handler_pbcol\n",
      "                        Progression Handler: Progress Reported as an\n",
      "                        ANSI Background Color in the Terminal\n",
      "progressr::handler_pbmcapply\n",
      "                        Progression Handler: Progress Reported via\n",
      "                        'pbmcapply' Progress Bars (Text) in the\n",
      "                        Terminal\n",
      "progressr::handler_progress\n",
      "                        Progression Handler: Progress Reported via\n",
      "                        'progress' Progress Bars (Text) in the Terminal\n",
      "progressr::handler_rpushbullet\n",
      "                        Progression Handler: Progress Reported via the\n",
      "                        Pushbullet Messaging Service\n",
      "progressr::handler_rstudio\n",
      "                        Progression Handler: Progress Reported in the\n",
      "                        RStudio Console\n",
      "progressr::handler_shiny\n",
      "                        Progression Handler: Progress Reported via\n",
      "                        'shiny' Widgets (GUI) in the HTML Browser\n",
      "progressr::handler_slowdown\n",
      "                        Progression Handler: Slow Down Progress\n",
      "                        Reporting\n",
      "progressr::handler_tkprogressbar\n",
      "                        Progression Handler: Progress Reported as a\n",
      "                        Tcl/Tk Progress Bars in the GUI\n",
      "progressr::handler_txtprogressbar\n",
      "                        Progression Handler: Progress Reported as Plain\n",
      "                        Progress Bars (Text) in the Terminal\n",
      "progressr::handler_void\n",
      "                        Progression Handler: No Progress Report\n",
      "progressr::handler_winprogressbar\n",
      "                        Progression Handler: Progress Reported as a MS\n",
      "                        Windows Progress Bars in the GUI\n",
      "progressr::make_progression_handler\n",
      "                        Creates a Progression Calling Handler\n",
      "  Aliases: make_progression_handler\n",
      "progressr::progress     Creates and Signals a Progression Condition\n",
      "progressr::progress_aggregator\n",
      "                        Aggregate Progression Conditions\n",
      "progressr::progression\n",
      "                        A Progression Condition\n",
      "  Aliases: progression\n",
      "progressr::register_global_progression_handler\n",
      "                        Add or Remove a Global 'progression' Handler\n",
      "  Aliases: register_global_progression_handler\n",
      "randomForest::combine   Combine Ensembles of Trees\n",
      "  Concepts: Regression\n",
      "randomForest::getTree   Extract a single tree from a forest.\n",
      "  Concepts: Regression and Classification Trees\n",
      "randomForest::grow      Add trees to an ensemble\n",
      "  Concepts: Regression\n",
      "randomForest::importance\n",
      "                        Extract variable importance measure\n",
      "  Concepts: Regression, Regression and Classification Trees\n",
      "randomForest::MDSplot   Multi-dimensional Scaling Plot of Proximity\n",
      "                        matrix from randomForest\n",
      "  Concepts: Regression and Classification Trees\n",
      "randomForest::partialPlot\n",
      "                        Partial dependence plot\n",
      "  Concepts: Regression, Regression and Classification Trees\n",
      "randomForest::plot.randomForest\n",
      "                        Plot method for randomForest objects\n",
      "  Concepts: Regression, Regression and Classification Trees\n",
      "randomForest::predict.randomForest\n",
      "                        predict method for random forest objects\n",
      "  Concepts: Regression\n",
      "randomForest::randomForest\n",
      "                        Classification and Regression with Random\n",
      "                        Forest\n",
      "  Concepts: Regression, Regression and Classification Trees\n",
      "randomForest::rfcv      Random Forest Cross-Valdidation for feature\n",
      "                        selection\n",
      "  Concepts: Regression\n",
      "randomForest::rfImpute\n",
      "                        Missing Value Imputations by randomForest\n",
      "  Concepts: Regression, Regression and Classification Trees\n",
      "randomForest::treesize\n",
      "                        Size of trees in an ensemble\n",
      "  Concepts: Regression\n",
      "randomForest::tuneRF    Tune randomForest for the optimal mtry\n",
      "                        parameter\n",
      "  Concepts: Regression and Classification Trees\n",
      "randomForest::varImpPlot\n",
      "                        Variable Importance Plot\n",
      "  Concepts: Regression, Regression and Classification Trees\n",
      "randomForest::varUsed   Variables used in a random forest\n",
      "  Concepts: Regression and Classification Trees\n",
      "RcppArmadillo::fastLm   Bare-bones linear model fitting function\n",
      "  Concepts: regression, Regression\n",
      "rpart::labels.rpart     Create Split Labels For an Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::meanvar.rpart    Mean-Variance Plot for an Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::na.rpart         Handles Missing Values in an Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::path.rpart       Follow Paths to Selected Nodes of an Rpart\n",
      "                        Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::plot.rpart       Plot an Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::plotcp           Plot a Complexity Parameter Table for an Rpart\n",
      "                        Fit\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::post.rpart       PostScript Presentation Plot of an Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::predict.rpart    Predictions from a Fitted Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::print.rpart      Print an Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::printcp          Displays CP table for Fitted Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::prune.rpart      Cost-complexity Pruning of an Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::residuals.rpart\n",
      "                        Residuals From a Fitted Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::rpart            Recursive Partitioning and Regression Trees\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::rpart.control    Control for Rpart Fits\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::rpart.exp        Initialization function for exponential fitting\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::rpart.object     Recursive Partitioning and Regression Trees\n",
      "                        Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::rsq.rpart        Plots the Approximate R-Square for the\n",
      "                        Different Splits\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::snip.rpart       Snip Subtrees of an Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::summary.rpart    Summarize a Fitted Rpart Object\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::text.rpart       Place Text on a Dendrogram Plot\n",
      "  Concepts: Regression and Classification Trees\n",
      "rpart::xpred.rpart      Return Cross-Validated Predictions\n",
      "  Concepts: Regression and Classification Trees\n",
      "splines::splines-package\n",
      "                        Regression Spline Functions and Classes\n",
      "stats::anova            ANOVA Tables\n",
      "  Concepts: regression, Regression\n",
      "stats::anova.glm        Analysis of Deviance for Generalized Linear\n",
      "                        Model Fits\n",
      "  Concepts: Regression\n",
      "stats::anova.lm         ANOVA for Linear Model Fits\n",
      "  Concepts: Regression\n",
      "stats::anova.mlm        Comparisons between Multivariate Linear Models\n",
      "  Concepts: Regression\n",
      "stats::aov              Fit an Analysis of Variance Model\n",
      "  Concepts: Regression\n",
      "stats::ar               Fit Autoregressive Models to Time Series\n",
      "  Concepts: autoregression\n",
      "stats::arima.sim        Simulate from an ARIMA Model\n",
      "  Concepts: autoregression\n",
      "stats::case.names       Case and Variable Names of Fitted Models\n",
      "  Concepts: Regression\n",
      "stats::coef             Extract Model Coefficients\n",
      "  Concepts: Regression\n",
      "stats::contr.helmert    (Possibly Sparse) Contrast Matrices\n",
      "  Concepts: Regression\n",
      "stats::contrasts        Get and Set Contrast Matrices\n",
      "  Concepts: Regression\n",
      "stats::dendrogram       General Tree Structures\n",
      "  Concepts: Regression and Classification Trees\n",
      "stats::deriv            Symbolic and Algorithmic Derivatives of Simple\n",
      "                        Expressions\n",
      "  Concepts: Non-linear Regression\n",
      "stats::df.residual      Residual Degrees-of-Freedom\n",
      "  Concepts: Regression\n",
      "stats::effects          Effects from Fitted Model\n",
      "  Concepts: Regression\n",
      "stats::expand.model.frame\n",
      "                        Add new variables to a model frame\n",
      "  Concepts: Regression\n",
      "stats::family.glm       Accessing Generalized Linear Model Fits\n",
      "  Concepts: Regression\n",
      "stats::family.lm        Accessing Linear Model Fits\n",
      "  Concepts: Regression\n",
      "stats::fitted           Extract Model Fitted Values\n",
      "  Concepts: Regression\n",
      "stats::getInitial       Get Initial Parameter Estimates\n",
      "  Concepts: Non-linear Regression\n",
      "stats::glm              Fitting Generalized Linear Models\n",
      "  Concepts: regression, Regression\n",
      "stats::influence.measures\n",
      "                        Regression Deletion Diagnostics\n",
      "  Concepts: Regression\n",
      "stats::isoreg           Isotonic / Monotone Regression\n",
      "  Concepts: monotonic regression, Regression\n",
      "stats::ksmooth          Kernel Regression Smoother\n",
      "stats::line             Robust Line Fitting\n",
      "  Concepts: Regression\n",
      "stats::lm               Fitting Linear Models\n",
      "  Concepts: regression, Regression\n",
      "stats::lm.fit           Fitter Functions for Linear Models\n",
      "  Concepts: Regression\n",
      "stats::lm.influence     Regression Diagnostics\n",
      "  Concepts: Regression\n",
      "stats::loess            Local Polynomial Regression Fitting\n",
      "stats::ls.diag          Compute Diagnostics for 'lsfit' Regression\n",
      "                        Results\n",
      "  Concepts: Regression\n",
      "stats::ls.print         Print 'lsfit' Regression Results\n",
      "  Concepts: Regression\n",
      "stats::lsfit            Find the Least Squares Fit\n",
      "  Concepts: Regression\n",
      "stats::nlm              Non-Linear Minimization\n",
      "  Concepts: Non-linear Regression\n",
      "stats::nls              Nonlinear Least Squares\n",
      "  Concepts: Non-linear Regression, Regression\n",
      "stats::nls.control      Control the Iterations in 'nls'\n",
      "  Concepts: Non-linear Regression, Regression\n",
      "stats::NLSstAsymptotic\n",
      "                        Fit the Asymptotic Regression Model\n",
      "stats::optim            General-purpose Optimization\n",
      "  Concepts: Non-linear Regression\n",
      "stats::plot.lm          Plot Diagnostics for an 'lm' Object\n",
      "  Concepts: Regression\n",
      "stats::plot.ppr         Plot Ridge Functions for Projection Pursuit\n",
      "                        Regression Fit\n",
      "stats::plot.profile.nls\n",
      "                        Plot a 'profile.nls' Object\n",
      "  Concepts: Non-linear Regression, Regression\n",
      "stats::ppr              Projection Pursuit Regression\n",
      "  Concepts: Regression\n",
      "stats::predict.glm      Predict Method for GLM Fits\n",
      "  Concepts: regression, Regression\n",
      "stats::predict.lm       Predict method for Linear Model Fits\n",
      "  Concepts: regression, Regression\n",
      "stats::predict.nls      Predicting from Nonlinear Least Squares Fits\n",
      "  Concepts: Non-linear Regression, Regression\n",
      "stats::profile.glm      Method for Profiling 'glm' Objects\n",
      "  Concepts: Regression\n",
      "stats::profile.nls      Method for Profiling 'nls' Objects\n",
      "  Concepts: Non-linear Regression, Regression\n",
      "stats::residuals        Extract Model Residuals\n",
      "  Concepts: Regression\n",
      "stats::stat.anova       GLM ANOVA Statistics\n",
      "  Concepts: Regression\n",
      "stats::summary.aov      Summarize an Analysis of Variance Model\n",
      "  Concepts: Regression\n",
      "stats::summary.glm      Summarizing Generalized Linear Model Fits\n",
      "  Concepts: Regression\n",
      "stats::summary.lm       Summarizing Linear Model Fits\n",
      "  Concepts: Regression\n",
      "stats::summary.nls      Summarizing Non-Linear Least-Squares Model Fits\n",
      "  Concepts: Regression\n",
      "stats::termplot         Plot Regression Terms\n",
      "  Concepts: Regression\n",
      "stats::vcov             Calculate Variance-Covariance Matrix for a\n",
      "                        Fitted Model Object\n",
      "  Concepts: Non-linear Regression\n",
      "stats::weighted.residuals\n",
      "                        Compute Weighted Residuals\n",
      "  Concepts: Regression\n",
      "survival::aareg         Aalen's additive regression model for censored\n",
      "                        data\n",
      "survival::anova.coxph   Analysis of Deviance for a Cox model.\n",
      "  Concepts: Regression\n",
      "survival::cch           Fits proportional hazards regression model to\n",
      "                        case-cohort data\n",
      "survival::clogit        Conditional logistic regression\n",
      "survival::cox.zph       Test the Proportional Hazards Assumption of a\n",
      "                        Cox Regression\n",
      "survival::coxph         Fit Proportional Hazards Regression Model\n",
      "survival::coxph.object\n",
      "                        Proportional Hazards Regression Object\n",
      "survival::coxphms.object\n",
      "                        Multi-state Proportional Hazards Regression\n",
      "                        Object\n",
      "survival::ridge         Ridge regression\n",
      "survival::survreg       Regression for a Parametric Survival Model\n",
      "survival::survreg.object\n",
      "                        Parametric Survival Model Object\n",
      "  Concepts: Regression\n",
      "urca::.spcv             Critical values for Schmidt and Phillips Unit\n",
      "                        Root Test\n",
      "  Concepts: Regression\n",
      "urca::ablrtest          Likelihood ratio test for restrictions on alpha\n",
      "                        and beta\n",
      "  Concepts: Regression\n",
      "urca::alphaols          OLS regression of VECM weighting matrix\n",
      "  Concepts: Regression\n",
      "urca::alrtest           Likelihood ratio test for restrictions on alpha\n",
      "  Concepts: Regression\n",
      "urca::bh5lrtest         Likelihood ratio test for restrictions under\n",
      "                        partly known beta\n",
      "  Concepts: Regression\n",
      "urca::bh6lrtest         Likelihood ratio test for restrictions under\n",
      "                        partly known beta in a subspace\n",
      "  Concepts: Regression\n",
      "urca::blrtest           Likelihood ratio test for restrictions on beta\n",
      "  Concepts: Regression\n",
      "urca::ca.jo             Johansen Procedure for VAR\n",
      "  Concepts: Regression\n",
      "urca::ca.po             Phillips and Ouliaris Cointegration Test\n",
      "  Concepts: Regression\n",
      "urca::cajolst           Testing Cointegrating Rank with Level Shift at\n",
      "                        Unknown time\n",
      "  Concepts: Regression\n",
      "urca::cajools           OLS regression of VECM\n",
      "  Concepts: Regression\n",
      "urca::cajorls           OLS regression of VECM\n",
      "  Concepts: Regression\n",
      "urca::lttest            Likelihood ratio test for no linear trend in\n",
      "                        VAR\n",
      "  Concepts: Regression\n",
      "urca::plotres           Graphical inspection of VECM residuals\n",
      "  Concepts: Regression\n",
      "urca::ur.df             Augmented-Dickey-Fuller Unit Root Test\n",
      "  Concepts: Regression\n",
      "urca::ur.ers            Elliott, Rothenberg and Stock Unit Root Test\n",
      "  Concepts: Regression\n",
      "urca::ur.kpss           Kwiatkowski et al. Unit Root Test\n",
      "  Concepts: Regression\n",
      "urca::ur.pp             Phillips and Perron Unit Root Test\n",
      "  Concepts: Regression\n",
      "urca::ur.sp             Schmidt and Phillips Unit Root Test\n",
      "  Concepts: Regression\n",
      "urca::ur.za             Zivot and Andrews Unit Root Test\n",
      "  Concepts: Regression\n",
      "\n",
      "\n",
      "Type '?PKG::FOO' to inspect entries 'PKG::FOO', or 'TYPE?PKG::FOO' for\n",
      "entries like 'PKG::FOO-TYPE'.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(glm)      # Equivalente a ?glm\n",
    "??regression   # Busca funciones relacionadas con \"regresiÃ³n\" en todos tus paquetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2547d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Consejo profesional en IA**\n",
    "La documentaciÃ³n de R es famosa por ser muy tÃ©cnica. No te agobies si no entiendes todo al principio. Ve directo a la secciÃ³n de **Examples** al final de la pÃ¡gina; suele ser la forma mÃ¡s rÃ¡pida de entender cÃ³mo aplicar un modelo a tus datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3fab3",
   "metadata": {},
   "source": [
    "### ðŸ”§ Ejercicio 5: Explorando la Variabilidad\n",
    "\n",
    "En Machine Learning, la **desviaciÃ³n estÃ¡ndar** nos indica quÃ© tan dispersos estÃ¡n nuestros datos (clave para detectar *outliers*).\n",
    "\n",
    "1. Abre la ayuda de la funciÃ³n `sd()`.\n",
    "2. Busca en la secciÃ³n **Arguments** quÃ© significa el parÃ¡metro `na.rm`.\n",
    "3. **Pregunta bonus:** Imagina que tienes un dataset con edades y falta un dato. Si `na.rm` es `FALSE` (valor por defecto), Â¿quÃ© crees que pasarÃ¡ al calcular la desviaciÃ³n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc7074f8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sd                    package:stats                    R Documentation\n",
      "\n",
      "_\bS_\bt_\ba_\bn_\bd_\ba_\br_\bd _\bD_\be_\bv_\bi_\ba_\bt_\bi_\bo_\bn\n",
      "\n",
      "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
      "\n",
      "     This function computes the standard deviation of the values in\n",
      "     â€˜xâ€™.  If â€˜na.rmâ€™ is â€˜TRUEâ€™ then missing values are removed before\n",
      "     computation proceeds.\n",
      "\n",
      "_\bU_\bs_\ba_\bg_\be:\n",
      "\n",
      "     sd(x, na.rm = FALSE)\n",
      "     \n",
      "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
      "\n",
      "       x: a numeric vector or an R object but not a â€˜factorâ€™ coercible\n",
      "          to numeric by â€˜as.double(x)â€™.\n",
      "\n",
      "   na.rm: logical.  Should missing values be removed?\n",
      "\n",
      "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
      "\n",
      "     Like â€˜varâ€™ this uses denominator n - 1.\n",
      "\n",
      "     The standard deviation of a length-one or zero-length vector is\n",
      "     â€˜NAâ€™.\n",
      "\n",
      "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
      "\n",
      "     â€˜varâ€™ for its square, and â€˜madâ€™, the most robust alternative.\n",
      "\n",
      "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
      "\n",
      "     sd(1:2) ^ 2\n",
      "     "
     ]
    }
   ],
   "source": [
    "# TODO: completa\n",
    "?sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a9a9d",
   "metadata": {},
   "source": [
    "Si na.rm es falso, no se quitarÃ­an esos valores que no existen, por tanto, si se considerasen cÃ³mo 0, se estarÃ­a variando la desviaciÃ³n estÃ¡ndar, por lo que el valor no serÃ­a correspondiente de las muestras que si que existen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb9583",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Mini-reto â€” NormalizaciÃ³n de un valor (IA)\n",
    "\n",
    "En Machine Learning, si intentas comparar el \"Precio de una casa\" (ej: 300,000) con el \"NÃºmero de habitaciones\" (ej: 3), el modelo se confundirÃ¡ por la diferencia de escalas. Para solucionarlo, usamos la **NormalizaciÃ³n Min-Max**, que comprime todos los valores a un rango entre **0 y 1**.\n",
    "\n",
    "La fÃ³rmula matemÃ¡tica es:\n",
    "\n",
    "$\n",
    "valorNormalizado = \\frac{valor - minimo}{maximo - minimo}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "###  Enunciado\n",
    "\n",
    "\n",
    "Imagina que estÃ¡s normalizando la columna de \"Ingresos\" de un dataset:\n",
    "\n",
    "1. **Crea tres variables**:\n",
    "* `valor`: AsÃ­gnale un ingreso de `50000`.\n",
    "* `minimo`: El ingreso mÃ­nimo en tu dataset es `10000`.\n",
    "* `maximo`: El ingreso mÃ¡ximo es `90000`.\n",
    "\n",
    "\n",
    "2. **Calcula el valor normalizado**: Usa la fÃ³rmula anterior y guarda el resultado en una variable llamada `valor_norm`. *Â¡Cuidado con los parÃ©ntesis!*\n",
    "3. **Muestra el resultado**: Usa la funciÃ³n `print()` y `paste()` para mostrar un mensaje tipo: *\"El valor normalizado es: [resultado]\"*.\n",
    "4. **VerificaciÃ³n**: Comprueba con `class()` que el resultado sea de tipo `numeric`.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Tip de experto\n",
    "\n",
    "En R, cuando trabajas con datasets reales, no harÃ¡s esto valor por valor. AplicarÃ¡s esta misma lÃ³gica a **vectores enteros** (columnas) para transformar miles de datos en una sola lÃ­nea de cÃ³digo.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7854e13e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.5\n"
     ]
    }
   ],
   "source": [
    "# TODO: completa\n",
    "valor <- 50000\n",
    "maximo <- 90000\n",
    "minimo <- 10000\n",
    "\n",
    "valorNormalizado <- ((valor - minimo) / (maximo - minimo))\n",
    "\n",
    "print(valorNormalizado)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
